{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "\n",
    "*Please fill out the relevant cells below according to the instructions. When done, save the notebook and export it to PDF, upload both the `ipynb` and the PDF file to Canvas.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Members\n",
    "\n",
    "*Group submission is highly encouraged. If you submit as part of group, list all group members here. Groups can comprise up to 5 students.*\n",
    "\n",
    "* Adam Applegate\n",
    "* Beatrix Brahms\n",
    "* \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Hubble goes GP\n",
    "\n",
    "You will remember that Edwin Hubble produced the first observational evidence for the expansion of the universe by linear regression of velocities and distances of nearby galaxies. One main limitation with his analysis is the lack of error estimates for any of the observed quantities. In Assignment 1, we assumed that all observed distances have and unknown but *the same* error. Here we will use the flexibility of Gaussian Processes instead.\n",
    "\n",
    "### Problem 1.1 (3pts):\n",
    "\n",
    "Load the data file `hubble_corrected.txt`. It's a version of the data from Assignment 1, Problem 4, that applies several astronomically motivated corrections, but still comes without error estimates.\n",
    "\n",
    "Perform GP regression of the relation $R(V)$ for noisy data with unknown variance from `scikit-learn` that we discussed during the lecture. Use `ConstantKernel * Matern + WhiteKernel` as kernel function and pick suitable values for their respective parameters. Plot the data, the regression mean and $\\pm1\\sigma$ confidence regions. Don't forget axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.2 (1pt):\n",
    "\n",
    "The approach in 1.1 suffices to support Hubble's original finding that the universe is in fact expanding, but it is not satisfying for two reasons:\n",
    "\n",
    "1. The GP mean function is not a straight line, in contrast to our (well: Hubble's) theoretical prejudice.\n",
    "2. The variance of the data is assumed to be constant and uncorrelated: $\\Sigma_{y,i}=\\Sigma_y\\ \\forall i$.\n",
    "\n",
    "Let's say that we are certain that the data model is linear without intercept. Then we can remedy both of these problems by specifying the analytic likelihood as a stochastic process with two sets of RVs:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y(x) &= b x + e(x)\\\\\n",
    "e(x)&\\sim\\mathrm{GP}(\\mu(x), \\kappa(x,x'))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "That means we impose that data come from a linear model (with only a slope parameter and no intercept) and that the *residual errors* $e$ of the linear model come from a GP. That evidently addresses shortcoming 1, and it allows for a more flexible error model.\n",
    "\n",
    "To visualize the approach, let's break it up into two steps.\n",
    "\n",
    "1. Compute $e_i = R_i - \\tilde{b}_{MLE} V_i$, where $\\tilde{b}_{MLE}$ is the MLE of the ordinary linear regression. \n",
    "2. Perform GP regression of $e(V)$ with the same functional form of the kernel as in 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.3 (3pts): \n",
    "\n",
    "The linear regression errors $e$ from 1.2 appear locally clustered: they form groups in $(V,R)$. These correlations of the errors can be accounted for by the GP, but so far the linear regression does not \"know\" of this new modeling flexibility. Let's find a way to optimize the linear model *and* the GP simultaneously.\n",
    "\n",
    "The likelihood for the data $\\lbrace(x_1,y_1),\\dots,(x_N,y_N)\\rbrace$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(\\lbrace y_i\\rbrace\\mid\\lbrace x_i \\rbrace, b, \\theta) &= -\\frac{1}{2} e^\\top K_\\theta^{-1} e - \\frac{1}{2}\\ln|K_\\theta | - \\frac{N}{2}\\ln(2\\pi)\\\\\n",
    "&= -\\frac{1}{2}(y - b x)^\\top K_\\theta^{-1} (y - b x)- \\frac{1}{2}\\ln|K_\\theta | - \\frac{N}{2}\\ln(2\\pi)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $K_\\theta=\\kappa(x,x';\\theta)$ is the error covariance matrix, and $\\theta$ denotes the parameter vector of the kernel function. In other words, $K$ is the covariance matrix of the $e_i$, whose parameters we seek to determine while also determining $b$ through minimizing the $e_i$.\n",
    "\n",
    "For this to be meaningful we need to specify $\\kappa$. The deviations from the mean appear locally clustered, which suggest a distance-based kernel. Like above, we'll use the robust Mat√©rn 3/2 kernel:\n",
    "\n",
    "$$\n",
    "\\kappa(x,x'; \\alpha, \\tau) = \\alpha^2 \\left(1+\\frac{\\sqrt{3} d}{\\tau}\\right)\\exp\\left(-\\frac{\\sqrt{3} d}{\\tau}\\right)\n",
    "$$\n",
    "\n",
    "where $d=\\lVert x-x'\\rVert_2$. To account for some (unknown) constant noise level, the data covariance matrix $K$ contains a diagonal term $\\sigma_y^2\\mathbf{I}$ with the unknown noise variance $\\sigma_y^2$ (that's what the `WhiteKernel` above does):\n",
    "\n",
    "$$\n",
    "K_\\theta \\rightarrow K_Y \\equiv \\sigma_y^2\\mathbf{I} + \\kappa(x,x'; \\alpha, \\tau)\n",
    "$$\n",
    "\n",
    "Implement the kernel function and the log likelihood above. Instead of a grid search, compute the gradients of $\\log p$ wrt the parameters $b$, $\\sigma_y$, $\\alpha$, and $\\tau$ with `jax.grad`. Evaluate the gradients at $(b=0.0015, \\sigma_y=0.1, \\alpha=1, \\tau=100)$. \n",
    "\n",
    "**Hints:**  \n",
    "\n",
    "* `import jax.numpy as jnp`\n",
    "* If you cannot install `jax`, try [`autograd`](https://github.com/HIPS/autograd) (its predecessor, installable with `pip install autograd`). Its `grad` function works just the same, but there's no built-in just-in-time compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.4 (2pt):\n",
    "    \n",
    "Use the gradient function from 1.3 to perform first-order gradient descent. Choose suitable step sizes and parameter initializations. Plot the loss curve (negative log likelihood as a function of iteration counter) and terminate when it appears to have converged. \n",
    "\n",
    "**Hint**: Make sure that all parameters stay positive during every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.5 (1pt):\n",
    "\n",
    "Visualize the result:\n",
    "\n",
    "1. Plot the matrix $K_\\theta$ for the data with the parameters $\\tilde\\theta$ you have determined in 1.4.\n",
    "2. Plot the data, the prediction mean and the $\\pm1\\sigma$ confidence regions. For that, compute $e = y - \\tilde{b} x$, compute the mean $\\mu_*$ and covariance $\\Sigma_*$ of the GP for $e$. Note that the cross-covariance $K_{X*}$ does not contain a diagonal error term. The final prediction is then $\\tilde{y} = \\tilde{b} x + \\mu_* \\pm \\mathrm{Diag}\\left(\\Sigma_*\\right)^{1/2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
